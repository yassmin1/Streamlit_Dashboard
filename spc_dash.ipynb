{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "614e0714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Patch the starter to improve Windows support and fix the download button\n",
    "from pathlib import Path\n",
    "import textwrap, io, json\n",
    "\n",
    "\n",
    "base = Path(r\"C:\\Users\\ryassminh\\python_notebooks\\Dashboard\")\n",
    "\n",
    "# --- Update README with Windows-specific instructions ---\n",
    "readme_path = base.joinpath(\"README.md\")\n",
    "readme = readme_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "readme_path.write_text(textwrap.dedent(\"\"\"\n",
    "# Streamlit Feature Engineering Starter\n",
    "\n",
    "This template helps you **prototype features first** and then reuse them in a Streamlit dashboard.\n",
    "\n",
    "## How to run \n",
    "\n",
    "### 1) Create & activate a virtual environment\n",
    "\n",
    "**Windows PowerShell**\n",
    "```powershell\n",
    "python -m venv .venv\n",
    ".\\\\.venv\\\\Scripts\\\\Activate.ps1\n",
    "``` \n",
    "**Windows Command Prompt**\n",
    "#If activation is blocked, run once as admin:\n",
    "```cmd\n",
    "Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n",
    "```\n",
    "\n",
    " \"\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332d313b",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "#!python -m venv .venv\n",
    "!.\\\\.venv\\\\Scripts\\\\activate.bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "013a50cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting etl/etl_cbmc1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl/etl_cbmc1.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import duckdb  # optional\n",
    "except Exception:\n",
    "    duckdb = None\n",
    "\n",
    "# -------------------------\n",
    "# Config & constants\n",
    "# -------------------------\n",
    "\n",
    "REQUIRED_COLUMNS = [\n",
    "    \"C1_BANNER_ID\",\n",
    "    \"C1_ACADEMIC_PERIOD_DESC\",\n",
    "    \"C1_ACADEMIC_PERIOD\",\n",
    "    \"C1_CALENDAR_YEAR\",\n",
    "    \"C1_COLLEGE\",\n",
    "    \"C1_GENDER_DESC\",\n",
    "    \"C1_CURRENT_AGE\",\n",
    "    \"C1_FTIC_DC_DESC\",\n",
    "    \"C1_TYPE_MAJOR_DESC\",\n",
    "    \"C1_FTPT_COLLEGE_CENSUS\",\n",
    "    \"C1_THECB_ETHNICITY\",\n",
    "]\n",
    "\n",
    "REQUIRED_COLUMNS_STU=[\n",
    "    \"Term\", \n",
    "    \"Student ID\",\n",
    "    \"Major Desc\",\n",
    "                      \n",
    "]\n",
    "\n",
    "COLUMN_MAPPING = {\n",
    "    \"C1_CALENDAR_YEAR\": \"Calendar Year\",\n",
    "    \"C1_ACADEMIC_PERIOD_DESC\": \"Academic Period\",\n",
    "    \"C1_CBM_TERM_DESC\": \"Term\",\n",
    "    \"C1_COLLEGE\": \"SPC College\",\n",
    "    \"C1_GENDER_DESC\": \"Gender\",\n",
    "    \"C1_FTIC_DC_DESC\": \"Student Type\",\n",
    "    \"C1_TYPE_MAJOR_DESC\": \"Major Type\",\n",
    "    \"C1_FTPT_COLLEGE_CENSUS\": \"Full_Part Time\",\n",
    "    \"C1_THECB_ETHNICITY\": \"Ethnicity\",\n",
    "    \"C1_CURRENT_AGE\": \"Age\",\n",
    "    \"Major Desc\":\"Major\",\n",
    "}\n",
    "\n",
    "AGE_BINS = [0, 18, 25, 30, 35, 40, 50, 60, 200]\n",
    "AGE_LABELS = [\"Under 18\", \"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-49\", \"50-59\", \"60+\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ETLPaths:\n",
    "    raw_inputs: List[Path]            # explicit files OR directories to glob under\n",
    "    stu_inputs: List[Path]            # explicit files OR directories to glob under\n",
    "    out_parquet: Path                 # curated parquet output\n",
    "    out_csv: Path                     # curated csv output\n",
    "    duckdb_path: Optional[Path] = None  # optional duckdb file\n",
    "    duckdb_table: str = \"cbmc1_merged\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"cbmc1_etl\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "\n",
    "def _iter_input_files(inputs: Iterable[Path]) -> Iterable[Path]:\n",
    "    \"\"\"Yield CSV/XLS/XLSX files from paths (files or directories).\"\"\"\n",
    "    for p in inputs:\n",
    "        p = Path(p)\n",
    "        if p.is_dir():\n",
    "            # read all CSV + Excel in this directory (one-level glob)\n",
    "            for f in p.glob(\"*.csv\"):\n",
    "                yield f\n",
    "            for f in p.glob(\"*.xls*\"):\n",
    "                yield f\n",
    "        elif p.is_file():\n",
    "            if p.suffix.lower() in {\".csv\", \".xls\", \".xlsx\"}:\n",
    "                yield p\n",
    "        else:\n",
    "            logger.warning(\"Path does not exist: %s\", p)\n",
    "\n",
    "\n",
    "def _read_one_file(fp: Path, columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Read a single CSV/Excel, subset columns, robust to encodings and date parsing.\"\"\"\n",
    "    suffix = fp.suffix.lower()\n",
    "    try:\n",
    "        if suffix == \".csv\":\n",
    "            # fast-path: try utf-8, fall back to latin1\n",
    "            try:\n",
    "                df = pd.read_csv(fp, dtype=str, low_memory=False)\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(fp, dtype=str, encoding=\"latin1\", low_memory=False)\n",
    "        elif suffix in {\".xlsx\", \".xls\"}:\n",
    "            # engine auto-chooses; for xls you may need xlrd installed\n",
    "            df = pd.read_excel(fp, dtype=str)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {fp}\")\n",
    "\n",
    "        missing = [c for c in columns if c not in df.columns]\n",
    "        if missing:\n",
    "            logger.warning(\"Missing columns in %s: %s\", fp.name, missing)\n",
    "\n",
    "        keep = [c for c in columns if c in df.columns]\n",
    "        df = df[keep].copy()\n",
    "        df[\"__source_file\"] = fp.name  # lineage\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to read %s: %s\", fp, e)\n",
    "        # Return empty DF with required columns so concat is safe\n",
    "        return pd.DataFrame(columns=columns + [\"__source_file\"])\n",
    "\n",
    "\n",
    "def extract_merge(inputs: Iterable[Path], columns: List[str] = REQUIRED_COLUMNS) -> pd.DataFrame:\n",
    "    \"\"\"Extract: read many CSV/Excel â†’ vertical concat, subsetting columns early.\"\"\"\n",
    "    files = list(_iter_input_files(inputs))\n",
    "    if not files:\n",
    "        logger.error(\"No input files found.\")\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    logger.info(\"Found %d files. Reading...\", len(files))\n",
    "    dfs = [_read_one_file(fp, columns) for fp in files]\n",
    "    merged = pd.concat(dfs, ignore_index=True)\n",
    "    logger.info(\"Merged shape after extract: %s\", merged.shape)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def _age_from_dob(dob_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute exact age in years (month/day aware) from a single column (Series)\n",
    "    of DATE_OF_BIRTH values.\n",
    "    Works even if some dates are missing or invalid.\n",
    "    \"\"\"\n",
    "    # 1. Convert column to datetime (handles strings, Excel serials, NaNs)\n",
    "    dob = pd.to_datetime(dob_series, errors=\"coerce\")\n",
    "\n",
    "    # 2. Get today's date (scalar timestamp)\n",
    "    today = pd.Timestamp.today()\n",
    "\n",
    "    # 3. Extract components (safe for Series)\n",
    "    year = dob.dt.year\n",
    "    month = dob.dt.month\n",
    "    day = dob.dt.day\n",
    "\n",
    "    # 4. Compute base difference in years\n",
    "    diff = today.year - year\n",
    "\n",
    "    # 5. Subtract 1 if birthday hasn't occurred yet this year\n",
    "    had_birthday = (today.month > month) | ((today.month == month) & (today.day >= day))\n",
    "    age = diff - (~had_birthday).astype(\"Int64\")\n",
    "\n",
    "    # 6. Keep null where dob is NaT\n",
    "    return age.where(dob.notna()).astype(\"Int64\")\n",
    "\n",
    "\n",
    "\n",
    "def merge_student_data(\n",
    "    cbm_df: pd.DataFrame,\n",
    "    stu_df: pd.DataFrame,\n",
    "    left_keys: list[str] = ['C1_ACADEMIC_PERIOD', 'C1_BANNER_ID'],\n",
    "    right_keys: list[str] = ['Term', 'Student ID'],\n",
    "    how: str = 'left',\n",
    "    suffixes: tuple[str, str] = (\"_cbm\", \"_stu\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge CBM dataset with student dataset safely and cleanly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cbm_df : pd.DataFrame\n",
    "        The main CBM dataset.\n",
    "    stu_df : pd.DataFrame\n",
    "        The student data to join (lookup/enrichment).\n",
    "    left_keys : list[str]\n",
    "        Column names in cbm_df to join on.\n",
    "    right_keys : list[str]\n",
    "        Column names in stu_df to join on.\n",
    "    how : str\n",
    "        Type of join (default 'left').\n",
    "    suffixes : tuple[str, str]\n",
    "        Suffixes to apply to overlapping columns.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Merged DataFrame with duplicates removed based on left_keys.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Merging on {left_keys} â†” {right_keys} using '{how}' join...\")\n",
    "\n",
    "        merged = pd.merge(\n",
    "            cbm_df,\n",
    "            stu_df,\n",
    "            left_on=left_keys,\n",
    "            right_on=right_keys,\n",
    "            how=how,\n",
    "            suffixes=suffixes\n",
    "        )\n",
    "\n",
    "        before = len(merged)\n",
    "        merged = merged.drop_duplicates(subset=left_keys, keep='first')\n",
    "        after = len(merged)\n",
    "\n",
    "        logger.info(f\"Merge complete. Rows: {before} â†’ {after} after deduplication.\")\n",
    "        return merged.reset_index(drop=True)\n",
    "\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"KeyError during merge: {e}\")\n",
    "        missing_keys = [k for k in left_keys if k not in cbm_df.columns] + \\\n",
    "                       [k for k in right_keys if k not in stu_df.columns]\n",
    "        raise KeyError(f\"Missing join keys: {missing_keys}\") from e\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Unexpected error during merge:\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def transform_cbmc1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform: the merged dataframe\n",
    "      - rename columns\n",
    "      - compute Age + Age_Group (and drop raw DOB & Age)\n",
    "      - trim whitespace, standardize categories\n",
    "      - drop all-null rows\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty dataframe passed to transform; returning as-is.\")\n",
    "        return df\n",
    "\n",
    "    # Standardize column names & trim strings\n",
    "    df = df.drop_duplicates().copy()\n",
    "    # basic whitespace cleanup\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # rename only existing columns\n",
    "    existing_map = {k: v for k, v in COLUMN_MAPPING.items() if k in df.columns}\n",
    "    df = df.rename(columns=existing_map)\n",
    "\n",
    "    # age + age group\n",
    "    if \"Age\" in df.columns:\n",
    "        age = pd.to_numeric(df[\"Age\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"Age_Group\"] = pd.cut(\n",
    "            age.astype(\"float\"),\n",
    "            bins=AGE_BINS,\n",
    "            labels=AGE_LABELS,\n",
    "            include_lowest=True,\n",
    "            right=False,\n",
    "        )\n",
    "        df = df.drop(columns=[\"Age\"], errors=\"ignore\")\n",
    "        # If you want to keep exact age, uncomment:\n",
    "        # df[\"Age\"] = age\n",
    "\n",
    "    # Drop rows completely empty\n",
    "    df = df.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Optional: cast some columns to category for smaller Parquet\n",
    "    for cat_col in [\"Term\", \"SPC College\", \"Gender\", \"Student Type\", \"Type_Major\", \"Full_Part_Time\", \"ETHNICITY\", \"Age_Group\",\"Major\"]:\n",
    "        if cat_col in df.columns:\n",
    "            df[cat_col] = df[cat_col].astype(\"category\")\n",
    "\n",
    "    logger.info(\"Transformed shape: %s | Columns: %s\", df.shape, list(df.columns))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_parquet(df: pd.DataFrame, out_path_par: Path,out_path_csv: Path) -> Path:\n",
    "    out_path_par.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # filter out feature not need to show \n",
    "    col_delete=['__source_file_cbm','C1_BANNER_ID','__source_file_stu','Student ID','SPC College','C1_ACADEMIC_PERIOD']\n",
    "    df = df.drop(columns=col_delete, errors=\"ignore\").dropna().reset_index(drop=True)\n",
    "    # Use pyarrow by default if available\n",
    "    \n",
    "    df.to_parquet(out_path_par, index=False)\n",
    "    df.to_csv(out_path_csv, index=False)\n",
    "    logger.info(\"Wrote curated Parquet â†’ %s (rows=%s)\", out_path_par, len(df))\n",
    "    return \"save files in data/curated\"\n",
    "\n",
    "\n",
    "def load_duckdb(df: pd.DataFrame, db_path: Path, table: str) -> Optional[Path]:\n",
    "    if duckdb is None:\n",
    "        logger.warning(\"duckdb not installed; skipping DuckDB load.\")\n",
    "        return None\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = duckdb.connect(str(db_path))\n",
    "    try:\n",
    "        con.register(\"df_mem\", df)\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE {table} AS SELECT * FROM df_mem;\")\n",
    "        logger.info(\"Wrote DuckDB table %s in %s\", table, db_path)\n",
    "    finally:\n",
    "        con.close()\n",
    "    return db_path\n",
    "\n",
    "\n",
    "def run_etl(paths: ETLPaths) -> Path:\n",
    "    raw_df = extract_merge(paths.raw_inputs, REQUIRED_COLUMNS) # Extract + merge\n",
    "    stu_df = extract_merge(paths.stu_inputs, REQUIRED_COLUMNS_STU) #\n",
    "    input_df = merge_student_data(\n",
    "    cbm_df=raw_df,\n",
    "    stu_df=stu_df,\n",
    "    left_keys=['C1_ACADEMIC_PERIOD', 'C1_BANNER_ID'],\n",
    "    right_keys=['Term', 'Student ID'],\n",
    "    how='left') # Merge CBM + Student data\n",
    "    curated = transform_cbmc1(input_df) #\n",
    "    out = load_parquet(curated, paths.out_parquet,paths.out_csv.with_suffix('.csv'))# Load Parquet (and CSV)\n",
    "    if paths.duckdb_path:\n",
    "        load_duckdb(curated, paths.duckdb_path, paths.duckdb_table)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "\n",
    "def _parse_args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"CBMC1 ETL: merge CSV/Excel â†’ Parquet (and optional DuckDB).\")\n",
    "    p.add_argument(\n",
    "        \"--cbmc_inputs\",\n",
    "        nargs=\"+\",\n",
    "        required=True,\n",
    "        help=\"Files or directories (space-separated). Directories will be scanned for *.csv and *.xls*\", \n",
    "        \n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--stu_inputs\",\n",
    "        nargs=\"+\",\n",
    "        required=True,\n",
    "        help=\"Files or directories (space-separated). Directories will be scanned for *.csv and *.xls*\", \n",
    "    \n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--out_parquet\",\n",
    "        required=True,\n",
    "        help=\"Path to curated parquet, e.g. data/curated/merged.parquet\",\n",
    "    )    \n",
    "    p.add_argument(\n",
    "        \"--out_csv\",\n",
    "        required=True,\n",
    "        help=\"Path to curated csv, e.g. data/curated/merged.csv\",    \n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--duckdb\",\n",
    "        default=None,\n",
    "        help=\"Optional DuckDB file path, e.g. warehouse/warehouse.duckdb\",\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--duckdb-table\",\n",
    "        default=\"cbmc1_merged\",\n",
    "        help=\"DuckDB table name (default: cbmc1_merged)\",\n",
    "    )\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = _parse_args()\n",
    "    paths = ETLPaths(\n",
    "        raw_inputs=[Path(p) for p in args.cbmc_inputs],\n",
    "        stu_inputs=[Path(p) for p in args.stu_inputs],\n",
    "        out_parquet=Path(args.out_parquet),\n",
    "        out_csv=Path(args.out_csv),\n",
    "        duckdb_path=Path(args.duckdb) if args.duckdb else None,\n",
    "        duckdb_table=args.duckdb_table,\n",
    "    )\n",
    "    run_etl(paths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c29b0",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 10:14:06,096 | INFO     | Found 1 files. Reading...\n",
      "2025-10-08 10:14:08,794 | INFO     | Merged shape after extract: (28609, 12)\n",
      "2025-10-08 10:14:08,796 | INFO     | Found 1 files. Reading...\n",
      "2025-10-08 10:14:09,986 | INFO     | Merged shape after extract: (53758, 4)\n",
      "2025-10-08 10:14:09,986 | INFO     | Merging on ['C1_ACADEMIC_PERIOD', 'C1_BANNER_ID'] â†” ['Term', 'Student ID'] using 'left' join...\n",
      "2025-10-08 10:14:10,019 | INFO     | Merge complete. Rows: 28609 â†’ 14535 after deduplication.\n",
      "2025-10-08 10:14:10,116 | INFO     | Transformed shape: (14535, 16) | Columns: ['C1_BANNER_ID', 'Academic Period', 'C1_ACADEMIC_PERIOD', 'Calendar Year', 'SPC College', 'Gender', 'Student Type', 'Major Type', 'Full_Part Time', 'Ethnicity', '__source_file_cbm', 'Term', 'Student ID', 'Major', '__source_file_stu', 'Age_Group']\n",
      "2025-10-08 10:14:10,231 | INFO     | Wrote curated Parquet â†’ data\\curated\\data_merged.parquet (rows=14535)\n",
      "2025-10-08 10:14:10,295 | INFO     | Wrote DuckDB table cbmc1_merged in warehouse\\warehouse.duckdb\n"
     ]
    }
   ],
   "source": [
    "!python etl/etl_cbmc1.py \\\n",
    "  --cbmc_inputs  data/cbmc1/\"cbmc1_Fall23_25.csv\" \\\n",
    "  --stu_inputs  data/stu220/\"STU0220_Fall24_25.csv\" \\\n",
    "  --out_parquet data/curated/data_merged.parquet \\\n",
    "  --out_csv data/curated/data_merged.csv \\\n",
    "  --duckdb warehouse/warehouse.duckdb \\\n",
    "  --duckdb-table cbmc1_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd7141",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Dashboard application code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d8945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting App.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile App.py\n",
    "#https://student-insights.streamlit.app/\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "#from etl.etl_cbmc1 import  ETLPaths, run_etl\n",
    "\n",
    "##############################################\n",
    "COLUMNS=[\n",
    "\"Academic Period\",\n",
    "\"Calendar Year\",\n",
    "\"Gender\",\n",
    "\"Student Type\",\n",
    "\"Full_Part Time\",\n",
    "\"Ethnicity\",\n",
    "\"Major\",\n",
    "\"Age_Group\",\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "def create_bar_chart(data, x_col, y_col, title=\"Bar Chart\", color_col=None, \n",
    "                     orientation='vertical', color_scheme='viridis'):\n",
    "    \"\"\"\n",
    "    Create a customizable bar chart for Streamlit dashboard\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with the data\n",
    "    - x_col: Column name for x-axis\n",
    "    - y_col: Column name for y-axis (values)\n",
    "    - title: Chart title\n",
    "    - color_col: Column name for color grouping (optional)\n",
    "    - orientation: 'vertical' or 'horizontal'\n",
    "    - color_scheme: Color palette ('viridis', 'blues', 'reds', etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if orientation == 'horizontal':\n",
    "        fig = px.bar(data, x=y_col, y=x_col, \n",
    "                     color=color_col if color_col else None,\n",
    "                     orientation='h',\n",
    "                     title=title,\n",
    "                     color_discrete_sequence=px.colors.qualitative.Set3)\n",
    "    else:\n",
    "        fig = px.bar(data, x=x_col, y=y_col, \n",
    "                     color=color_col if color_col else None,\n",
    "                     title=title,\n",
    "                     color_discrete_sequence=px.colors.qualitative.Set3)\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(\n",
    "        title_font_size=20,\n",
    "        title_x=0.5,  # Center the title\n",
    "        xaxis_title_font_size=14,\n",
    "        yaxis_title_font_size=14,\n",
    "        font_size=12,\n",
    "        plot_bgcolor='rgba(0,0,0,0)',  # Transparent background\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # Add hover information\n",
    "    fig.update_traces(\n",
    "        hovertemplate='<b>%{x}</b><br>Value: %{y}<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "###\n",
    "def create_stacked_bar_chart(data, x_col, y_cols, title=\"Stacked Bar Chart\"):\n",
    "    \"\"\"Create a stacked bar chart with multiple y-columns\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for col in y_cols:\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=col,\n",
    "            x=data[x_col],\n",
    "            y=data[col]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        barmode='stack',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_grouped_bar_chart(data, x_col, y_cols, title=\"Grouped Bar Chart\"):\n",
    "    \"\"\"Create a grouped bar chart with multiple y-columns\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for col in y_cols:\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=col,\n",
    "            x=data[x_col],\n",
    "            y=data[col]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        barmode='group',\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "# =============================================================================\n",
    "# CACHED FUNCTIONS FOR PERFORMANCE OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Cache the data loading function to avoid reloading data every time the app runs\n",
    "@st.cache_data\n",
    "def load_and_process_data(file_par,columns=COLUMNS):\n",
    "    \"\"\"\n",
    "    Load and process CBM data with caching for performance\n",
    "    \n",
    "    The @st.cache_data decorator ensures this function only runs once per session.\n",
    "    Subsequent calls will return the cached result, dramatically improving performance.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed CBM data or None if loading fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        sample_data=(pd.read_parquet(file_par)[columns])\n",
    "\n",
    "        \n",
    "        \n",
    "        return sample_data\n",
    "    except Exception as e:\n",
    "        # Display error message to user if data loading fails\n",
    "        st.error(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cache the value counts calculation to avoid recalculating for the same column\n",
    "@st.cache_data\n",
    "def get_value_counts(data, column):\n",
    "    \"\"\"\n",
    "    Get value counts for a specific column with caching\n",
    "    \n",
    "    This function calculates how many times each unique value appears in a column.\n",
    "    Caching prevents recalculation when the user switches between different view options\n",
    "    (like percentages vs counts) for the same column.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to analyze\n",
    "        column (str): Name of the column to count values for\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Value counts for the specified column\n",
    "    \"\"\"\n",
    "    return data[column].value_counts()\n",
    "\n",
    "# Cache cross-tabulation calculations to improve performance for category comparisons\n",
    "@st.cache_data\n",
    "def get_cross_tabulation(data, col1, col2):\n",
    "    \"\"\"\n",
    "    Get cross-tabulation between two columns with caching\n",
    "    \n",
    "    Cross-tabulation shows the relationship between two categorical variables\n",
    "    by counting occurrences of each combination. Caching this expensive operation\n",
    "    prevents recalculation when users switch between visualization options.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset to analyze\n",
    "        col1 (str): First categorical column\n",
    "        col2 (str): Second categorical column\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cross-tabulation table showing relationships between categories\n",
    "    \"\"\"\n",
    "    ct = pd.crosstab(data[col1], data[col2])\n",
    "    ct = ct[ct.sum(axis=0).sort_values(ascending=False).index]\n",
    "    \n",
    "    return ct\n",
    "\n",
    "\n",
    "def sidebar_major_selector(df: pd.DataFrame,\n",
    "                           major_col: str = \"Major\",\n",
    "                           title: str = \"ðŸŽ“ Majors\",\n",
    "                           key_prefix: str = \"majors\") -> list[str]:\n",
    "    \"\"\"Sidebar widget: 'Select All' + multiselect for majors.\n",
    "\n",
    "    - De-dupes & sorts options\n",
    "    - Preserves selection in session state\n",
    "    - Disables the multiselect when 'Select All' is checked\n",
    "    - Guards against empty selections\n",
    "    \"\"\"\n",
    "    # Build stable, clean options\n",
    "    majors = (\n",
    "        df[major_col]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .loc[lambda s: s.ne(\"\")]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    majors = sorted(set(majors))  # de-dup + sort once\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.subheader(title)\n",
    "        # Select-all checkbox\n",
    "        all_key = f\"{key_prefix}_select_all\"\n",
    "        multi_key = f\"{key_prefix}_multiselect\"\n",
    "\n",
    "        select_all = st.checkbox(\"Select all majors\", value=True, key=all_key)\n",
    "\n",
    "        if select_all:\n",
    "            # Show disabled multiselect for clarity (everything selected)\n",
    "            st.multiselect(\n",
    "                \"Majors\",\n",
    "                options=majors,\n",
    "                default=majors,\n",
    "                key=multi_key,\n",
    "                disabled=True,\n",
    "                help=\"All majors are included.\"\n",
    "            )\n",
    "            selected = majors\n",
    "        else:\n",
    "            # Use prior selection if exists; otherwise default to all\n",
    "            default_vals = st.session_state.get(multi_key, majors)\n",
    "            selected = st.multiselect(\n",
    "                \"Majors\",\n",
    "                options=majors,\n",
    "                default=default_vals,\n",
    "                key=multi_key,\n",
    "                help=\"Uncheck 'Select all majors' to filter.\"\n",
    "            )\n",
    "            # Guard: if user clears everything, keep empty but warn\n",
    "            if len(selected) == 0:\n",
    "                st.info(\"No majors selected â€” results may be empty.\")\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN DASHBOARD FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that creates the Streamlit dashboard for SPC data analysis.\n",
    "    \n",
    "    This function sets up the entire user interface, loads data, creates visualizations,\n",
    "    and handles user interactions. It's optimized for performance with large datasets\n",
    "    through caching, sampling, and smart data limiting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure the Streamlit page with title and wide layout for better space utilization\n",
    "    st.set_page_config(page_title=\"Student Insights\", layout=\"wide\")\n",
    "    col1, col2 = st.columns([1, 4])  \n",
    "    with col1:\n",
    "        # Display the SPC logo from local assets\n",
    "        st.image(\"assets/analysis.png\", width=200)\n",
    "        \n",
    "    with col2:     \n",
    "        # Create the main title and separator line\n",
    "        st.title(\"Student Insights\", anchor=None)\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            <div style='color:#555;font-size:20px; line-height:1.4;margin-top:-10px'>\n",
    "              <strong>An interactive dashboard designed to explore student enrollment, demographics, and program trends. </strong>\n",
    "              <br>\n",
    "              <strong> Use the sidebar filters to select academic terms, majors, and student characteristics to uncover patterns that support data-informed decisions. </strong>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            unsafe_allow_html=True,\n",
    "        )\n",
    "    st.markdown(\"---\")  # Creates a horizontal line for visual separation\n",
    "    \n",
    "    # =============================================================================\n",
    "    # DATA LOADING SECTION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Load data with caching - this spinner shows while data is loading\n",
    "    # The spinner improves user experience by indicating that something is happening\n",
    "    with st.spinner(\"Loading data...\"):\n",
    "        sample_data = load_and_process_data(\"data/curated/data_merged.parquet\")\n",
    "    \n",
    "    # Check if data loading was successful\n",
    "    if sample_data is None or sample_data.empty:\n",
    "        # Display error message and stop execution if no data available\n",
    "        st.error(\"Failed to load data. Please check the file path and try again.\")\n",
    "        return\n",
    "    \n",
    "    # =============================================================================\n",
    "    # SESSION STATE OPTIMIZATION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Store frequently used data information in session state to avoid repeated calculations\n",
    "    # Session state persists across user interactions, improving performance\n",
    "    if 'data_info' not in st.session_state:\n",
    "        st.session_state.data_info = {\n",
    "            'total_records': len(sample_data),  # Total number of rows in dataset\n",
    "            'columns': sample_data.columns.tolist(),  # All column names\n",
    "            # Only categorical columns (object/category types) are suitable for counting analysis\n",
    "            'categorical_columns': sample_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        }\n",
    "    \n",
    "    # Use cached categorical columns list for better performance\n",
    "    categorical_columns = st.session_state.data_info['categorical_columns']\n",
    "    \n",
    "    # =============================================================================\n",
    "    # SIDEBAR CONTROLS SECTION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Create sidebar for user controls - keeps main area clean for visualizations\n",
    "    st.sidebar.header(\"Analysis Controls\")\n",
    "    \n",
    "    # Dropdown to select which categorical column to analyze\n",
    "    # Index=0 means the first column is selected by default\n",
    "    selected_column = st.sidebar.selectbox(\n",
    "        \"Select Category to Analyze\",\n",
    "        categorical_columns,\n",
    "        index=0\n",
    "    )\n",
    "    \n",
    "    # Radio buttons for chart orientation - affects how bars are displayed\n",
    "    orientation = st.sidebar.radio(\n",
    "        \"Chart Orientation\",\n",
    "        [\"vertical\", \"horizontal\"]  # vertical = bars go up, horizontal = bars go sideways\n",
    "    )\n",
    "    \n",
    "    #checkbox to taggle between Terms\n",
    "    show_terms = st.sidebar.multiselect(\n",
    "        \"Select Terms to Include\",\n",
    "        options=sample_data[\"Academic Period\"].unique(),\n",
    "        default=sample_data[\"Academic Period\"].unique().tolist()  # Select all terms by default\n",
    "    )\n",
    "    #checkbox to taggle between Majors\n",
    "    show_majors = sidebar_major_selector(sample_data, major_col=\"Major\")\n",
    "    \n",
    "    # Checkbox to toggle between showing counts vs percentages\n",
    "    show_percentages = st.sidebar.checkbox(\"Show Percentages\", value=False)\n",
    "    \n",
    "    #----------------------------\n",
    "    #fiter based on select terms \n",
    "    #--------------------------------\n",
    "    if show_terms:\n",
    "        sample_data = sample_data[sample_data[\"Academic Period\"].isin(show_terms) & sample_data['Major'].isin(show_majors)]\n",
    "    else:\n",
    "        st.warning(\"Please select at least one term to display data.\")\n",
    "        return\n",
    "    #-----------------------\n",
    "    \n",
    "    # =============================================================================\n",
    "    # PERFORMANCE OPTIMIZATION FOR LARGE DATASETS\n",
    "    # =============================================================================\n",
    "    \n",
    "    # If dataset is very large (>10,000 records), offer sampling option for better performance\n",
    "    if len(sample_data) > 10000:\n",
    "        # Inform user about large dataset and optimization options\n",
    "        st.sidebar.info(f\"Dataset has {len(sample_data):,} records. Using optimized processing.\")\n",
    "        \n",
    "        # Checkbox to enable/disable sampling\n",
    "        use_sampling = st.sidebar.checkbox(\"Use sampling for faster processing\", value=True)\n",
    "        \n",
    "        if use_sampling:\n",
    "            # Use maximum of 5,000 records or the full dataset size, whichever is smaller\n",
    "            sample_size = min(5000, len(sample_data))\n",
    "            # random_state=42 ensures reproducible sampling\n",
    "            display_data = sample_data.sample(n=sample_size, random_state=42)\n",
    "            st.sidebar.info(f\"Using sample of {sample_size:,} records\")\n",
    "        else:\n",
    "            # Use full dataset if sampling is disabled\n",
    "            display_data = sample_data\n",
    "    else:\n",
    "        # For smaller datasets, use all data\n",
    "        display_data = sample_data\n",
    "        \n",
    "        \n",
    "    \n",
    "    # =============================================================================\n",
    "    # MAIN DASHBOARD LAYOUT\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Create two columns: left for chart (2/3 width), right for statistics (1/3 width)\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # LEFT COLUMN: MAIN VISUALIZATION\n",
    "    # =============================================================================\n",
    "    \n",
    "    with col1:\n",
    "        # Get cached value counts for the selected column\n",
    "        # This prevents recalculation when user changes visualization options\n",
    "        value_counts = get_value_counts(display_data, selected_column)\n",
    "        \n",
    "        # Limit categories for performance - showing too many bars makes charts unreadable\n",
    "        if len(value_counts) > 20:\n",
    "            st.info(f\"Showing top 20 categories out of {len(value_counts)} total\")\n",
    "            value_counts = value_counts.head(20)  # Keep only top 20 most frequent categories\n",
    "        \n",
    "        # Convert value counts to DataFrame format required by Plotly\n",
    "        count_data = value_counts.reset_index()\n",
    "        count_data.columns = [selected_column, 'Count']  # Rename columns for clarity\n",
    "        \n",
    "        # Calculate percentages if user requested them\n",
    "        if show_percentages:\n",
    "            # Calculate percentage of each category relative to total\n",
    "            count_data['Percentage'] = (count_data['Count'] / count_data['Count'].sum()) * 100\n",
    "            y_col = 'Percentage'  # Use percentage column for y-axis\n",
    "            title = f\"Distribution of {selected_column} (%)\"\n",
    "            # Custom hover template showing both count and percentage\n",
    "            hover_template = '<b>%{x}</b><br>Count: %{customdata}<br>Percentage: %{y:.1f}%<extra></extra>'\n",
    "            customdata = count_data['Count']  # Show actual counts in hover\n",
    "        else:\n",
    "            # Use raw counts\n",
    "            y_col = 'Count'\n",
    "            title = f\"Count of Each {selected_column}\"\n",
    "            # Simpler hover template for count-only display\n",
    "            hover_template = '<b>%{x}</b><br>Count: %{y}<extra></extra>'\n",
    "            customdata = None\n",
    "        \n",
    "        # Create bar chart using Plotly Express (faster than Graph Objects)\n",
    "        if orientation == 'horizontal':\n",
    "            # Horizontal bar chart: x-axis = values, y-axis = categories\n",
    "            fig = px.bar(\n",
    "                count_data, \n",
    "                x=y_col,  # Values (count or percentage)\n",
    "                y=selected_column,  # Categories\n",
    "                orientation='h',  # 'h' = horizontal bars\n",
    "                title=title,\n",
    "                color=y_col,  # Color bars by their height (creates gradient effect)\n",
    "                color_continuous_scale='viridis'  # Professional color scheme\n",
    "            )\n",
    "        else:\n",
    "            # Vertical bar chart: x-axis = categories, y-axis = values\n",
    "            fig = px.bar(\n",
    "                count_data, \n",
    "                x=selected_column,  # Categories\n",
    "                y=y_col,  # Values (count or percentage)\n",
    "                title=title,\n",
    "                color=y_col,  # Color bars by their height\n",
    "                color_continuous_scale='viridis'\n",
    "            )\n",
    "        \n",
    "        # Customize chart appearance for better performance and aesthetics\n",
    "        fig.update_layout(\n",
    "            title_font_size=16,  # Readable title size\n",
    "            height=450,  # Fixed height for consistent layout\n",
    "            showlegend=False,  # Remove color legend to save space and improve performance\n",
    "            plot_bgcolor='rgba(0,0,0,0)',  # Transparent plot background\n",
    "            paper_bgcolor='rgba(0,0,0,0)'  # Transparent paper background\n",
    "        )\n",
    "        \n",
    "        # Add custom hover information\n",
    "        if customdata is not None:\n",
    "            # When showing percentages, include both count and percentage in hover\n",
    "            fig.update_traces(customdata=customdata, hovertemplate=hover_template)\n",
    "        else:\n",
    "            # When showing counts only, use simpler hover template\n",
    "            fig.update_traces(hovertemplate=hover_template)\n",
    "        \n",
    "        # Add text annotations on bars only for small datasets to avoid clutter\n",
    "        if len(count_data) <= 15:\n",
    "            if show_percentages:\n",
    "                # Show both count and percentage on bars\n",
    "                text_values = [f\"{count}<br>({pct:.1f}%)\" for count, pct in \n",
    "                              zip(count_data['Count'], count_data['Percentage'])]\n",
    "            else:\n",
    "                # Show only counts on bars\n",
    "                text_values = count_data['Count'].astype(str)\n",
    "            \n",
    "            # Position text outside bars for vertical, inside for horizontal orientation\n",
    "            fig.update_traces(\n",
    "                text=text_values,\n",
    "                textposition='outside' if orientation == 'vertical' else 'inside'\n",
    "            )\n",
    "        \n",
    "        # Display the chart using full container width\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # RIGHT COLUMN: SUMMARY STATISTICS AND DATA TABLE\n",
    "    # =============================================================================\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"Summary Statistics\")\n",
    "        \n",
    "        # Get summary statistics using cached data info for better performance\n",
    "        #total_count = st.session_state.data_info['total_records']  # Total records in original dataset\n",
    "\n",
    "        total_count = len(display_data) # Total records in original dataset\n",
    "        unique_categories = display_data[selected_column].nunique()  # Number of unique values\n",
    "        \n",
    "        # Handle case where value_counts might be empty (error prevention)\n",
    "        if not value_counts.empty:\n",
    "            most_common = value_counts.index[0]  # Most frequent category\n",
    "            most_common_count = value_counts.iloc[0]  # Count of most frequent category\n",
    "        else:\n",
    "            most_common = \"N/A\"\n",
    "            most_common_count = 0\n",
    "        \n",
    "        # Display key metrics using Streamlit's metric widget for nice formatting\n",
    "        st.metric(\"Total Records\", f\"{total_count:,}\")  # :, adds thousand separators\n",
    "        st.metric(\"Unique Categories\", f\"{unique_categories}\")\n",
    "        st.metric(\"Most Common\", str(most_common))  # Convert to string for display\n",
    "        st.metric(\"Most Common Count\", f\"{most_common_count:,}\")\n",
    "        \n",
    "        # Show frequency table with the data\n",
    "        st.subheader(\"Frequency Table\")\n",
    "        freq_table = count_data.copy()  # Copy to avoid modifying original data\n",
    "        \n",
    "        # Round percentages if they exist\n",
    "        if 'Percentage' in freq_table.columns:\n",
    "            freq_table['Percentage'] = freq_table['Percentage'].round(1)\n",
    "        \n",
    "        # Limit table size for performance - tables with many rows are slow to render\n",
    "        if len(freq_table) > 10:\n",
    "            freq_table_display = freq_table.head(10)  # Show only top 10 rows\n",
    "            st.info(f\"Showing top 10 out of {len(freq_table)} categories\")\n",
    "        else:\n",
    "            freq_table_display = freq_table\n",
    "            \n",
    "        def auto_height(df, row_height=35, max_height=600):\n",
    "            return min(len(df) * row_height + 40, max_height)\n",
    "        # Display the frequency table with fixed height for consistent layout\n",
    "        st.dataframe(freq_table_display, use_container_width=True, height=auto_height(freq_table_display))\n",
    "    \n",
    "    # =============================================================================\n",
    "    # CROSS-CATEGORY ANALYSIS SECTION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Add visual separator and section header\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Cross-Category Analysis\")\n",
    "    \n",
    "    # Disable cross-analysis for very large datasets to maintain performance\n",
    "    if len(display_data) > 50000:\n",
    "        st.warning(\"Cross-category analysis disabled for large datasets to maintain performance.\")\n",
    "        return  # Exit function early\n",
    "    \n",
    "    # Create two columns for category selection\n",
    "    col_a, col_b = st.columns(2)\n",
    "    \n",
    "    with col_a:\n",
    "        # Dropdown for first category in comparison\n",
    "        category_1 = st.selectbox(\"First Category\", categorical_columns, index=0)\n",
    "    \n",
    "    with col_b:\n",
    "        # Dropdown for second category, defaulting to second column if available\n",
    "        if len(categorical_columns) > 1:\n",
    "            default_index = 1 if len(categorical_columns) > 1 else 0\n",
    "            category_2 = st.selectbox(\"Second Category\", categorical_columns, index=default_index)\n",
    "        else:\n",
    "            # Show warning if not enough categorical columns for cross-analysis\n",
    "            st.warning(\"Need at least 2 categorical columns for cross-analysis\")\n",
    "            return  # Exit function early\n",
    "    \n",
    "    # Only proceed if user selected two different categories\n",
    "    if category_1 != category_2:\n",
    "        # Check if cross-tabulation would be too large (performance safeguard)\n",
    "        unique_cat1 = display_data[category_1].nunique()  # Number of unique values in first category\n",
    "        unique_cat2 = display_data[category_2].nunique()  # Number of unique values in second category\n",
    "        \n",
    "        # Limit cross-tabulation size to prevent performance issues\n",
    "        if unique_cat1 * unique_cat2 > 500:  # 500 cells is reasonable limit\n",
    "            st.warning(f\"Cross-tabulation too large ({unique_cat1} x {unique_cat2}). \"\n",
    "                      \"Please select categories with fewer unique values.\")\n",
    "            return  # Exit function early\n",
    "        \n",
    "        # Create two columns for cross-analysis visualization and data\n",
    "        col3, col4 = st.columns(2)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # LEFT COLUMN: CROSS-TABULATION VISUALIZATION\n",
    "        # =============================================================================\n",
    "        \n",
    "        with col3:\n",
    "            # Get cached cross-tabulation to avoid recalculation\n",
    "            cross_tab = get_cross_tabulation(display_data, category_1, category_2)\n",
    "            \n",
    "            # Limit categories shown in visualization for readability and performance\n",
    "            if len(cross_tab.index) > 10:\n",
    "                cross_tab = cross_tab.head(10)  # Keep only top 10 rows\n",
    "                st.info(\"Showing top 10 categories for performance\")\n",
    "            \n",
    "            if len(cross_tab.columns) > 10:\n",
    "                cross_tab = cross_tab.iloc[:, :10]  # Keep only first 10 columns\n",
    "                st.info(\"Showing top 10 subcategories for performance\")\n",
    "            \n",
    "            # Create stacked bar chart using Plotly Express for better performance\n",
    "            # Transpose (.T) the data for better visualization\n",
    "            fig2 = px.bar(\n",
    "                cross_tab.T,  # Transpose so categories become x-axis\n",
    "                title=f\"{category_1} vs {category_2} Distribution\",\n",
    "                height=400  # Fixed height for consistency\n",
    "            )\n",
    "            \n",
    "            # Customize chart layout\n",
    "            fig2.update_layout(\n",
    "                xaxis_title=category_2,  # X-axis shows second category\n",
    "                yaxis_title=\"Count\",     # Y-axis shows counts\n",
    "                showlegend=True,         # Show legend for first category\n",
    "                legend_title=category_1  # Legend title is first category\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Display the cross-tabulation chart\n",
    "            st.plotly_chart(fig2, use_container_width=True)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # RIGHT COLUMN: CROSS-TABULATION DATA TABLE\n",
    "        # =============================================================================\n",
    "        \n",
    "        with col4:\n",
    "            st.subheader(\"Cross-Tabulation\")\n",
    "            \n",
    "            # Show preview for large tables to maintain performance\n",
    "            if cross_tab.size > 100:  # If table has more than 100 cells\n",
    "                st.info(\"Showing preview of cross-tabulation\")\n",
    "                # Show only 5x5 preview of the full table\n",
    "                st.dataframe(cross_tab.iloc[:5, :5], use_container_width=True)\n",
    "            else:\n",
    "                # Show full table for smaller cross-tabulations\n",
    "                st.dataframe(cross_tab, use_container_width=True)\n",
    "            \n",
    "            # Show summary statistics instead of full proportions table (better performance)\n",
    "            st.subheader(\"Summary\")\n",
    "            st.write(f\"**Categories in {category_1}:** {len(cross_tab.index)}\")\n",
    "            st.write(f\"**Categories in {category_2}:** {len(cross_tab.columns)}\")\n",
    "            st.write(f\"**Total Combinations:** {cross_tab.sum().sum():,}\")  # Total count across all cells\n",
    "\n",
    "# =============================================================================\n",
    "# SCRIPT ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main function when script is executed directly\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a4385c",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run App.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
